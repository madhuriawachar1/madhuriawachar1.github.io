[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madhuri Awachar",
    "section": "",
    "text": "Hi! I am Madhuri Awachar.As an enthusiastic learner, I am always eager to take up challenging tasks and push my limits to achieve excellence. My academic journey has give me a strong foundation in the concepts of computer science, and I have gained practical experience through various projects.Currently i am pursuing M.Tech from IIT Gandhinagar, Gujarat, India."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Madhuri Awachar",
    "section": "",
    "text": "M.Tech in Computer Science and Engineering\nAliah university , kol | july 2019-sep 2021\nB.Tech in Computer Science and Engineering\nAliah university , kol | july 2015-july 2019"
  },
  {
    "objectID": "index.html#publication",
    "href": "index.html#publication",
    "title": "Madhuri Awachar",
    "section": "",
    "text": "Descriptive Predictive Model for Parkinson’s Disease Analysis"
  },
  {
    "objectID": "index.html#awards",
    "href": "index.html#awards",
    "title": "Madhuri Awachar",
    "section": "",
    "text": "1st division scholarship in 10th.\naward in chemistary for excelent student.\nObtained Rank 1 in the university final exam in B.tech 2019.\nObtained Rank 2 in the university final exam in M.tech 2021.\nObtained rank 1 AuAT2019\nObtained rank 64 in AUAT2015\nWBJEE AND AIEEE Qualified in 2015."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ACHIEVEMENTS",
    "section": "",
    "text": "• Obtained Second Rank in the University final exam in Master of technology(M.TECH ).\n• Obtain First Rank in AUAT2019.\n• Obtained First Rank in the University final exam in Bachelor of technology (B.TECH ).\n• Obtain 64 Rank in AUAT2015.\n• BCECE Qualified in 2015.\n• WB JEE & AIEEE Qualified in 2015.\n• National scholarship.\nThree times in B.tech and two times in M.tech.\n• 1st division scholaship\nMukhyamantri Balak/Balika protsahan yojana.\n• Scholarship\n1st prize in Chemistry."
  },
  {
    "objectID": "index.html#project",
    "href": "index.html#project",
    "title": "Madhuri Awachar",
    "section": "",
    "text": "Prediction of parkinson disease.\nSmart home automation"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Setting Up Development Environment\n\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nKhush Shah\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Conference.html",
    "href": "Conference.html",
    "title": "Conference",
    "section": "",
    "text": "Descriptive Predictive Model for Parkinson’s Disease Analysis"
  },
  {
    "objectID": "workshop.html",
    "href": "workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "Participated in National Seminar on “Preparation & Publication of Research Articles” organized by Department of Computer Science and Engineering, Aliah University, Kolkata on 18th March 2019.\nParticipated in National Seminar on “Mathematical Modelling and its Application in Natural and Engineering Science (NSMMANES- 2019)” organized by Department of Mathematics and Statistics, Aliah University, Kolkata on 25th March 2019."
  },
  {
    "objectID": "Project.html",
    "href": "Project.html",
    "title": "Project",
    "section": "",
    "text": "Maintenance Portal (Prof. Mayank Singh, IITGn)-Github\n\nDesigned and constructed a robust and user-friendly Maintenance Portal using Flask, a popular Python web framework\nTook charge of both frontend and backend responsibilities, ensuring seamless integration and optimal system functionality by closely collaborating with the frontend team during the supervision of the entire backend development for the project\nIdentified existing limitations, executed essential enhancements to boost performance and user experience\nEmployed MySQL as the database management system, ensuring efficient data storage and retrieval\n\nImplementation of Machine Learning algorithms from scratch (Prof. Nipun Batra, IITGn)\n\nDevised a recursive algorithm for decision trees from scratch, used entropy and Gini Index as splitting criteria - Github\nLeveraged Bagging and Boosting algorithms to enhance Random Forest classifier and regressor-Github\nOptimized Linear Regression using Gradient Descent, Singular Value Decomposition, and mathematical formulae-Github\nEstablished a backend movie recommendation system utilizing crowd-sourced data with K Nearest Neighbours algorithm\nConstructed an application using streamlit to showcase how bagging reduces variance for a model for regression/classification-Link\n\nCoresets-based optimization of deep learning models (prof. Anirban Dasgupta, IITGN)\n\nCreated coresets using distinct methods and compared their performance(cost) by various models like k-means\nImplemented private K-means to develop private coresets that enables differentially private clustering on real data\n\nNGO Connect an Android project using Java (Prof. B.P.Vasgi, SCOE, Pune)\n\nDeveloped Android application intended to use as interface between NGOs, Hostels/restaurants and people who want to serve for NGO. It included services like serve on weekend, food for nearby\nApplied various software development methodologies, including agile, to successfully manage and deliver the project\nIncorporated essential Firebase services, including Firebase Cloud Messaging, Firebase Authentication, Real-time Database, Firebase Storage, and Firebase Hosting, to enhance the NGO Connect Android app’s functionality and user experience\nLeveraged Firebase’s push notification technology to enhance user engagement and communication in Android app development"
  },
  {
    "objectID": "Certification.html",
    "href": "Certification.html",
    "title": "image",
    "section": "",
    "text": "• Certified as “Elite” in the course “PHP WITH MYSQL” by WebTek Labs Pvt.Ltd. Jun-aug2018. • Certified as “Elite” in the course “Natural Language Processing(NLP) inpython” byudemy Online Certification Courses during Jan-Mar 2020. • Certified as “Solving International Quention” in “chegg expert learningplatform” by Chegg india Pvt. Ltd. Serial No. cert_dhg104nq. • Certified as “Q&A” in “chegg expert learning platform” by Chegg india Pvt.Ltd. Serial No. cert_cqbfzn14. • Certified as “Mastering the guidline” in “chegg expert learning platform” byChegg india Pvt. Ltd. Serial No. cert_172nvmdz. • Certified as “Elite” in the course “DCA” by OCSM Pvt. Ltd. Registration No.05/DCA/MZP-42/380/27717"
  },
  {
    "objectID": "gallery.html",
    "href": "gallery.html",
    "title": "Gallary",
    "section": "",
    "text": "IITGN Green Campus, Lal minar view\n\n\n\n\n\nMumbai"
  },
  {
    "objectID": "posts/firs.html",
    "href": "posts/firs.html",
    "title": "Setting Up Development Environment",
    "section": "",
    "text": "This is my personal website made with quarto and github pages.\nMy learnings/tasks while working on it:\nFor Windows:-\n\nInstall VS-Code\nInstall Git and connect/configure with GitHub along with VS-Code\nInstall Quarto in the system and Quarto extension in VS-Code\nCreate a new repository in GitHub\nCreate a new project in Quarto\nCreate proper “_quarto.yml”, “index.qmd” files\nRender the files and push to GitHub\nConfigure GitHub pages to publish the website\n\nFor Linux:-\n\nInstall “Windows Subsystem for Linux” (WSL) from Microsoft Store\nInstall “Ubuntu” as distributor\nIf “WslRegisterDistribution failed with error: 0x80370114” error comes, refer https://www.cyberithub.com/solved-wslregisterdistribution-failed-with-error-0x80370114\nInstall Quarto in the Linux system and Quarto extension in VS-Code\nRun command wget https://github.com/quarto-dev/quarto-cli/releases/download/v1.3.340/quarto-1.3.340-linux-amd64.deb in the Linux command Line Interface (CLI)\nRun command sudo dpkg -i quarto-1.3.340-linux-amd64.deb in the Linux Command Line Interface (CLI)\nRun command sudo apt-get install -f in the Linux command Line Interface (CLI)\nRun command quarto in the Linux command Line Interface (CLI) to check if Quarto is installed properly\nInstall Anaconda in the Linux system\nRun command wget https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Linux-x86_64.sh in the Linux command Line Interface (CLI)\nRun command bash Anaconda3-2023.03-1-Linux-x86_64.sh in the Linux command Line Interface (CLI)\nRun command ipython in the Linux command Line Interface (CLI) to check if Anaconda is installed properly\nConnect/configure with GitHub\nGo to https://github.com/settings/tokens to generate a new token and save it. It’ll be useful later\nClone the repository\nRun command git clone https://github.com/Khush24Shah/Khush24Shah.github.io in the Linux command Line Interface (CLI)\nEnter “username” and the “token” generated in the previous step"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Blogs\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nReccomandation System\n\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nMadhuri Awachar\n\n\n\n\n\n\n  \n\n\n\n\nCUR DEcomposition\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2023\n\n\nMadhuri Awachar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/Myfirst_blog.html",
    "href": "blogs/Myfirst_blog.html",
    "title": "Setting Up Development Environment",
    "section": "",
    "text": "This is my personal website made with quarto and github pages.\nMy learnings/tasks while working on it:\nFor Windows:-\n\nInstall VS-Code\nInstall Git and connect/configure with GitHub along with VS-Code\nInstall Quarto in the system and Quarto extension in VS-Code\nCreate a new repository in GitHub\nCreate a new project in Quarto\nCreate proper “_quarto.yml”, “index.qmd” files\nRender the files and push to GitHub\nConfigure GitHub pages to publish the website\n\nFor Linux:-\n\nInstall “Windows Subsystem for Linux” (WSL) from Microsoft Store\nInstall “Ubuntu” as distributor\nIf “WslRegisterDistribution failed with error: 0x80370114” error comes, refer https://www.cyberithub.com/solved-wslregisterdistribution-failed-with-error-0x80370114\nInstall Quarto in the Linux system and Quarto extension in VS-Code\nRun command wget https://github.com/quarto-dev/quarto-cli/releases/download/v1.3.340/quarto-1.3.340-linux-amd64.deb in the Linux command Line Interface (CLI)\nRun command sudo dpkg -i quarto-1.3.340-linux-amd64.deb in the Linux Command Line Interface (CLI)\nRun command sudo apt-get install -f in the Linux command Line Interface (CLI)\nRun command quarto in the Linux command Line Interface (CLI) to check if Quarto is installed properly\nInstall Anaconda in the Linux system\nRun command wget https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Linux-x86_64.sh in the Linux command Line Interface (CLI)\nRun command bash Anaconda3-2023.03-1-Linux-x86_64.sh in the Linux command Line Interface (CLI)\nRun command ipython in the Linux command Line Interface (CLI) to check if Anaconda is installed properly\nConnect/configure with GitHub\nGo to https://github.com/settings/tokens to generate a new token and save it. It’ll be useful later\nClone the repository\nRun command git clone https://github.com/Khush24Shah/Khush24Shah.github.io in the Linux command Line Interface (CLI)\nEnter “username” and the “token” generated in the previous step"
  },
  {
    "objectID": "blogs/MLLosses.html",
    "href": "blogs/MLLosses.html",
    "title": "Types of Losses and Optimisation.",
    "section": "",
    "text": "Loss or Objective Function is a measure of the model’s performance. It is optimised during the training to improve model’s performance.\nBroadly speaking, loss functions can be grouped into two major categories concerning the types of problems we come across in the real world: CLASSIFICATION and REGRESSION. In CLASSIFICATION problems, our task is to predict the respective probabilities of all classes the problem is dealing with. When it comes to REGRESSION, our task is to predict the continuous value concerning a given set of independent features to the learning algorithm."
  },
  {
    "objectID": "blogs/MLLosses.html#mean-absolute-error-loss",
    "href": "blogs/MLLosses.html#mean-absolute-error-loss",
    "title": "Types of Losses and Optimisation.",
    "section": "Mean Absolute Error Loss",
    "text": "Mean Absolute Error Loss\nWe define MAE loss function as the average of absolute differences between the actual and the predicted value. It’s the second most commonly used regression loss function. It measures the average magnitude of errors in a set of predictions, without considering their directions.\n\\[\\mathrm{MAE}(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y_i}|\\]\nwhere \\(y_i\\) is the actual value and \\(\\hat{y_i}\\) is the predicted value.\nThe corresponding cost function is the mean of these absolute errors (MAE). It is also known as the \\(\\ell_1\\) loss function."
  },
  {
    "objectID": "blogs/MLLosses.html#mean-squared-error-loss",
    "href": "blogs/MLLosses.html#mean-squared-error-loss",
    "title": "Types of Losses and Optimisation.",
    "section": "Mean Squared Error Loss",
    "text": "Mean Squared Error Loss\nWe define MSE loss function as the average of squared differences between the actual and the predicted value. It’s the most commonly used regression loss function.\n\\[\\mathrm{MSE}(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\]\nwhere \\(y_i\\) is the actual value and \\(\\hat{y_i}\\) is the predicted value.\nThe corresponding cost function is the mean of these squared errors (MSE). It is also known as the \\(\\ell_2\\) loss function. The MSE loss function penalizes the model for making large errors by squaring them."
  },
  {
    "objectID": "blogs/MLLosses.html#huber-loss",
    "href": "blogs/MLLosses.html#huber-loss",
    "title": "Types of Losses and Optimisation.",
    "section": "Huber Loss",
    "text": "Huber Loss\nWe define Huber loss function as the combination of MSE and MAE. It’s less sensitive to outliers than the MSE loss function and is differentiable at 0.\n\\[\\mathrm{Huber}(\\boldsymbol{y}, \\boldsymbol{\\hat{y}} ; \\delta) = \\frac{1}{n}\\sum_{i=1}^{n}\\mathrm{L}_{\\delta}(y_i - \\hat{y_i})\\]\n\\[\\mathrm{L}_{\\delta}(y_i - \\hat{y_i}) = \\begin{cases} \\frac{1}{2}(y_i - \\hat{y_i})^2 & \\text{for } |y_i - \\hat{y_i}| \\leq \\delta \\\\ \\delta|y_i - \\hat{y_i}| - \\frac{1}{2}\\delta^2 & \\mathrm{otherwise} \\end{cases}\\]\nwhere \\(y_i\\) is the actual value and \\(\\hat{y_i}\\) is the predicted value.\nThe corresponding cost function is the mean of these Huber errors. The Huber loss function is more robust to outliers compared to the MSE loss function."
  },
  {
    "objectID": "blogs/MLLosses.html#example",
    "href": "blogs/MLLosses.html#example",
    "title": "Types of Losses and Optimisation.",
    "section": "Example",
    "text": "Example\nLet’s take a simple example to understand the above loss functions.\nSay for some data, the actual value is 100 and the predicted value is 110. The loss function for the above loss functions will be:\n\\[\\begin{align*}\n\\mathrm{MAE} &= |100 - 110| &= 10 \\\\\n\\mathrm{MSE} &= (100 - 110)^2 &= 100 \\\\\n\\mathrm{Huber}(\\delta = 5) &= \\mathrm{L}_{\\delta}(100 - 110)\\\\\n&= 5 \\times |100 - 110| - \\frac{1}{2}\\times5^2 = 50 - 12.5 &= 37.5 \\\\\n\\end{align*}\\]\nHere, we can see that the MAE loss function is the least sensitive to outliers. The MSE loss function is the most sensitive to outliers. The Huber loss function is less sensitive to outliers than the MSE loss function and is differentiable at 0."
  },
  {
    "objectID": "blogs/MLLosses.html#implementation",
    "href": "blogs/MLLosses.html#implementation",
    "title": "Types of Losses and Optimisation.",
    "section": "Implementation",
    "text": "Implementation\n\nImporting Libraries\n\nimport plotly.graph_objects as go\nimport torch\nfrom torch.optim import Adam\n\nimport sklearn.metrics as metrics\n\n\n\nDefining the loss functions\n\n# MAE loss\ndef mae(y, y_pred, extra=None):\n    assert y.shape == y_pred.shape\n    val = torch.abs(y - y_pred)\n    return torch.mean(val)\n\n# MSE loss\ndef mse(y, y_pred, extra=None):\n    assert y.shape == y_pred.shape\n    val = (y - y_pred) ** 2\n    return torch.mean(val)\n\n# Huber loss\ndef huber(y, y_pred, extra=None):\n    assert y.shape == y_pred.shape\n    d = extra if extra else 1\n    diff = torch.abs(y - y_pred)\n    val = torch.where(diff &lt; d, 0.5 * diff ** 2, d * diff - 0.5 * d ** 2)\n    return torch.mean(val)\n\n# Binary Cross-Entropy loss\ndef bce(y, y_pred, extra=None):\n    assert y.shape == y_pred.shape\n    val = -y * torch.log(y_pred) - (1 - y) * torch.log(1 - y_pred)\n    return torch.mean(val)\n\n# Focal loss\ndef focal(y, y_pred, extra=None):\n    assert y.shape == y_pred.shape\n    g = extra if extra else 2\n    case_1 = -y * torch.log(y_pred) * (1 - y_pred) ** g\n    case_0 = -(1 - y) * torch.log(1 - y_pred) * y_pred ** g\n    val = case_1 + case_0\n    return torch.mean(val)\n\n\nloss_func = {\"mae\": mae, \"mse\": mse, \"huber\": huber, \"bce\": bce, \"focal\": focal}\n\n\n\nTraining Function\n\n# Train function\ndef train(x, y, w, loss_type, lr=0.01, epochs=100, extra=False):\n    if loss_type in (\"mae\", \"mse\", \"huber\"): # regression\n        classes = False\n        res = \"RMSE\"\n        res_func = metrics.mean_squared_error\n    elif loss_type in (\"bce\", \"focal\"): # classification\n        classes = True\n        res = \"Accuracy\"\n        res_func = metrics.accuracy_score\n    else:\n        raise ValueError(\"Unknown loss function\")\n    \n    result = []\n    loss_fn = loss_func[loss_type] # get loss function\n\n    opt = Adam([w], lr=lr)\n\n    for i in range(epochs):\n        y_pred = torch.matmul(x, w)\n        if classes: # classification\n            y_pred = torch.sigmoid(y_pred)\n\n        loss = loss_fn(y, y_pred, extra)\n        loss.backward()\n\n        opt.step()\n        opt.zero_grad()\n\n        if classes: # classification\n            y_pred = torch.where(y_pred &gt; 0.5, 1.0, 0.0) # threshold\n            result.append(res_func(y, y_pred.detach()))\n        else: # regression\n            result.append(res_func(y, y_pred.detach(), squared=False))\n\n        if i % (epochs // 10) == 0:\n            print(f'Epoch {i}, loss {loss:.4f}, {res} {result[-1]:.4f}')\n\n    return result, y_pred.detach()\n\n\n\nGenerating the data for regression\n\nx = torch.rand(500, 1)\ny = 2 * x + 3 + torch.randn(500, 1) * 0.5\n\nx = torch.concatenate([x, torch.ones((500, 1))], axis=1)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=y[:, 0], mode='markers', name='data'))\nfig.update_layout(title='Data', xaxis_title='x', yaxis_title='y')\nfig.show()\n\n\n                                                \n\n\n\n\nMAE Loss\n\nTraining\n\nw = torch.randn(2, 1, requires_grad=True)\n\nlr = 0.1\nloss_fn = \"mae\"\n\n\nresult, y_pred = train(x, y, w, loss_fn, lr=lr, epochs=200)\n\nEpoch 0, loss 5.0595, RMSE 5.1217\nEpoch 20, loss 2.1006, RMSE 2.1534\nEpoch 40, loss 0.5658, RMSE 0.7100\nEpoch 60, loss 0.4173, RMSE 0.5248\nEpoch 80, loss 0.3829, RMSE 0.4829\nEpoch 100, loss 0.3719, RMSE 0.4692\nEpoch 120, loss 0.3690, RMSE 0.4648\nEpoch 140, loss 0.3684, RMSE 0.4642\nEpoch 160, loss 0.3684, RMSE 0.4643\nEpoch 180, loss 0.3684, RMSE 0.4642\n\n\n\n\nPlot the loss\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=torch.arange(len(result)), y=result, mode='lines', name='loss'))\nfig.update_layout(title='Loss', xaxis_title='epoch', yaxis_title='loss')\nfig.show()\n\n\n                                                \n\n\n\n\nPlot the data with the regression line\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=y.ravel(), mode='markers', name='data'))\nfig.add_trace(go.Scatter(x=x[:, 0], y=2 * x[:, 0] + 3, mode='lines', name='true line', line=dict(color='green')))\nfig.add_trace(go.Scatter(x=x[:, 0], y=y_pred[:, 0], mode='lines', name='regression line', line=dict(color='red')))\nfig.show()\n\n\n                                                \n\n\n\n\n\nMSE Loss\n\nTraining\n\nw = torch.randn(2, 1, requires_grad=True)\n\nlr = 0.1\nloss_fn = \"mse\"\n\n\nresult, y_pred = train(x, y, w, \"mse\", lr=lr, epochs=200)\n\nEpoch 0, loss 33.1759, RMSE 5.7599\nEpoch 20, loss 8.3986, RMSE 2.8980\nEpoch 40, loss 0.8111, RMSE 0.9006\nEpoch 60, loss 0.2259, RMSE 0.4752\nEpoch 80, loss 0.2344, RMSE 0.4842\nEpoch 100, loss 0.2175, RMSE 0.4664\nEpoch 120, loss 0.2172, RMSE 0.4660\nEpoch 140, loss 0.2166, RMSE 0.4654\nEpoch 160, loss 0.2163, RMSE 0.4651\nEpoch 180, loss 0.2161, RMSE 0.4648\n\n\n\n\nPlot the loss\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=torch.arange(len(result)), y=result, mode='lines', name='loss'))\nfig.update_layout(title='Loss', xaxis_title='epoch', yaxis_title='loss')\nfig.show()\n\n\n                                                \n\n\n\n\nPlot the data with the regression line\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=y.ravel(), mode='markers', name='data'))\nfig.add_trace(go.Scatter(x=x[:, 0], y=2 * x[:, 0] + 3, mode='lines', name='true line', line=dict(color='green')))\nfig.add_trace(go.Scatter(x=x[:, 0], y=y_pred[:, 0], mode='lines', name='regression line', line=dict(color='red')))\nfig.update_layout(title='Data', xaxis_title='x', yaxis_title='y')\nfig.show()\n\n\n                                                \n\n\n\n\n\nHuber Loss\n\nTraining\n\nw = torch.randn(2, 1, requires_grad=True)\n\nlr = 0.1\nloss_fn = \"huber\"\n\n\nresult, y_pred = train(x, y, w, loss_fn, lr=lr, epochs=200, extra=0.5)\n\nEpoch 0, loss 0.9997, RMSE 2.3211\nEpoch 20, loss 0.1458, RMSE 0.6103\nEpoch 40, loss 0.1058, RMSE 0.4996\nEpoch 60, loss 0.0954, RMSE 0.4702\nEpoch 80, loss 0.0935, RMSE 0.4641\nEpoch 100, loss 0.0935, RMSE 0.4640\nEpoch 120, loss 0.0934, RMSE 0.4639\nEpoch 140, loss 0.0934, RMSE 0.4639\nEpoch 160, loss 0.0934, RMSE 0.4639\nEpoch 180, loss 0.0934, RMSE 0.4639\n\n\n\n\nPlot the loss\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=torch.arange(len(result)), y=result, mode='lines', name='loss'))\nfig.update_layout(title='Loss', xaxis_title='epoch', yaxis_title='loss')\nfig.show()\n\n\n                                                \n\n\n\n\nPlot the data with the regression line\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=y.ravel(), mode='markers', name='data'))\nfig.add_trace(go.Scatter(x=x[:, 0], y=2 * x[:, 0] + 3, mode='lines', name='true line', line=dict(color='green')))\nfig.add_trace(go.Scatter(x=x[:, 0], y=y_pred[:, 0], mode='lines', name='regression line', line=dict(color='red')))\nfig.show()"
  },
  {
    "objectID": "blogs/MLLosses.html#binary-cross-entropy-loss",
    "href": "blogs/MLLosses.html#binary-cross-entropy-loss",
    "title": "Types of Losses and Optimisation.",
    "section": "Binary Cross-Entropy Loss",
    "text": "Binary Cross-Entropy Loss\nThis is the most common loss function used in classification problems. The binary cross-entropy loss decreases as the predicted probability converges to the actual label. It measures the performance of a classification model whose predicted output is a probability value between 0 and 1.\n\\[\\mathrm{L}(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\begin{cases} -\\log(\\hat{y_i}) & \\text{if } y_i = 1 \\\\ -\\log(1-\\hat{y_i}) & \\text{if } y_i = 0 \\end{cases}\\]\n\\[\\mathrm{L}(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = - \\dfrac{1}{m} \\sum_{i=1}^{m} y_i \\log(\\hat{y_i}) + (1-y_i) \\log(1-\\hat{y_i})\\]\nwhere \\(y_i\\) is the actual value and \\(\\hat{y_i}\\) is the predicted value."
  },
  {
    "objectID": "blogs/MLLosses.html#focal-loss",
    "href": "blogs/MLLosses.html#focal-loss",
    "title": "Types of Losses and Optimisation.",
    "section": "Focal Loss",
    "text": "Focal Loss\nWe define Focal loss function as the combination of Binary Cross-Entropy Loss and a modulating factor. The modulating factor \\(\\gamma\\) is used to reduce the relative loss for well-classified examples and put more focus on hard, misclassified examples. It’s less sensitive to outliers than the Binary Cross-Entropy Loss function and is differentiable at 0.\n\\[\\mathrm{FL}(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\begin{cases} -(1-\\hat{y_i})^{\\gamma}\\log(\\hat{y_i}) & \\text{if } y_i = 1 \\\\ -(\\hat{y_i})^{\\gamma}\\log(1-\\hat{y_i}) & \\text{if } y_i = 0 \\end{cases}\\]\n\\[\\mathrm{FL}(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = - \\dfrac{1}{m} \\sum_{i=1}^{m} y_i (1 - \\hat{y_i})^{\\gamma} \\log(\\hat{y_i}) + (1-y_i) (\\hat{y_i})^{\\gamma} \\log(1-\\hat{y_i})\\]\nwhere \\(y_i\\) is the actual label and \\(\\hat{y_i}\\) is the predicted probability of the label."
  },
  {
    "objectID": "blogs/MLLosses.html#example-1",
    "href": "blogs/MLLosses.html#example-1",
    "title": "Types of Losses and Optimisation.",
    "section": "Example",
    "text": "Example\nLet’s take a simple example to understand the above loss functions.\nSay for some data, the actual label is 1 and the predicted probability of the label is 0.85. The loss function for the above loss functions will be:\n\\[\\begin{align*}\n\\mathrm{BCE} &= -\\log(0.85) &\\approx 0.162 \\\\\n\\mathrm{FL}(\\gamma = 2) &= -(1-0.85)^2\\log(0.85) &\\approx 0.004 \\\\\n\\end{align*}\\]\nAnd for some data, the actual label is 1 and the predicted probability of the label is 0.55. The loss function for the above loss functions will be:\n\\[\\begin{align*}\n\\mathrm{BCE} &= -\\log(0.55) &\\approx 0.598 \\\\\n\\mathrm{FL}(\\gamma = 2) &= -(1-0.55)^2\\log(0.55) &\\approx 0.121 \\\\\n\\end{align*}\\]\nHere, we can see that the propotional increase is approximately 3.6 times in the BCE loss function and approximately 30 times in the FL loss function. Hence, the FL loss function penalizes the model more for misclassifying the data."
  },
  {
    "objectID": "blogs/MLLosses.html#implementation-1",
    "href": "blogs/MLLosses.html#implementation-1",
    "title": "Types of Losses and Optimisation.",
    "section": "Implementation",
    "text": "Implementation\n\nGenerating the data for classification\n\nfrom sklearn.datasets import make_blobs\n\n\nx, y = make_blobs(n_samples=500, centers=2, cluster_std=2, random_state=42)\n\nx = torch.from_numpy(x).float()\ny = torch.from_numpy(y).float().reshape(-1, 1)\n\nx = torch.concatenate([x, torch.ones((500, 1))], axis=1)\n\ncolor = ['red' if l == 0 else 'blue' for l in y]\n\n# plot data\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=x[:, 1], mode='markers', marker=dict(color=color)))\nfig.update_layout(title='Data', xaxis_title='x', yaxis_title='y')\nfig.show()\n\n\n                                                \n\n\n\n\nBinary Cross-Entropy Loss\n\nTraining\n\nw = torch.randn(3, 1, requires_grad=True)\n\nlr = 0.1\nloss_fn = \"bce\"\n\n\nresult, y_pred = train(x, y, w, loss_fn, lr=lr, epochs=100)\n\nEpoch 0, loss 2.5939, Accuracy 0.4800\nEpoch 10, loss 0.2322, Accuracy 0.9520\nEpoch 20, loss 0.0507, Accuracy 0.9900\nEpoch 30, loss 0.0366, Accuracy 0.9860\nEpoch 40, loss 0.0330, Accuracy 0.9880\nEpoch 50, loss 0.0317, Accuracy 0.9900\nEpoch 60, loss 0.0310, Accuracy 0.9880\nEpoch 70, loss 0.0305, Accuracy 0.9880\nEpoch 80, loss 0.0301, Accuracy 0.9900\nEpoch 90, loss 0.0297, Accuracy 0.9900\n\n\n\n\nPlot the loss\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=torch.arange(len(result)), y=result, mode='lines', name='Accuracy'))\nfig.update_layout(title='Accuracy', xaxis_title='Epoch', yaxis_title='Accuracy')\nfig.show()\n\n\n                                                \n\n\n\n\nPlot the data with the separation line\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=x[:, 1], mode='markers', marker=dict(color=color)))\nfig.add_trace(go.Scatter(x=x[:, 0], y=(-(w[0] * x[:, 0] + w[2]) / w[1]).detach(), mode='lines', name='separation line', line=dict(color='red')))\nfig.update_layout(title='Data', xaxis_title='x', yaxis_title='y', yaxis_range=(x[:, 1].min() - 5, x[:, 1].max() + 5))\nfig.show()\n\n\n                                                \n\n\n\n\n\nFocal Loss\n\nTraining\n\nw = torch.randn(3, 1, requires_grad=True)\n\nlr = 0.1\nloss_fn = \"focal\"\n\n\nresult, y_pred = train(x, y, w, loss_fn, lr=lr, epochs=100, extra=5)\n\nEpoch 0, loss 1.3781, Accuracy 0.5020\nEpoch 10, loss 0.0047, Accuracy 0.9820\nEpoch 20, loss 0.0156, Accuracy 0.9720\nEpoch 30, loss 0.0191, Accuracy 0.9720\nEpoch 40, loss 0.0163, Accuracy 0.9740\nEpoch 50, loss 0.0125, Accuracy 0.9800\nEpoch 60, loss 0.0097, Accuracy 0.9860\nEpoch 70, loss 0.0078, Accuracy 0.9820\nEpoch 80, loss 0.0068, Accuracy 0.9820\nEpoch 90, loss 0.0061, Accuracy 0.9820\n\n\n\n\nPlot the loss\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=torch.arange(len(result)), y=result, mode='lines', name='Accuracy'))\nfig.update_layout(title='Accuracy', xaxis_title='Epoch', yaxis_title='Accuracy')\nfig.show()\n\n\n                                                \n\n\n\n\nPlot the data with the separation line\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=x[:, 1], mode='markers', marker=dict(color=color)))\nfig.add_trace(go.Scatter(x=x[:, 0], y=(-(w[0] * x[:, 0] + w[2]) / w[1]).detach(), mode='lines', name='separation line', line=dict(color='red')))\nfig.update_layout(title='Classification', xaxis_title='x', yaxis_title='y', yaxis_range=(x[:, 1].min() - 5, x[:, 1].max() + 5))\nfig.show()\n\n\n                                                \n\n\n\n\n\nComparison between BCE and FL\n\ndef ce(p):\n    return -torch.log(p)\n\ndef fl(p, gamma=2):\n    return (1 - p) ** gamma * -torch.log(p)\n\n\nx = torch.arange(0.01, 1, 0.01)\n\nfig = go.Figure()\nfor gamma in [0, 0.5, 1, 2, 5, 10]:\n    fig.add_trace(go.Scatter(x=x, y=fl(x, gamma)/ce(x), mode='lines', name=f'gamma={gamma}'))\nfig.update_layout(title='Focal Loss / Cross Entropy', xaxis_title='p', yaxis_title='Focal Loss / Cross Entropy')\nfig.show()\n\n\n                                                \n\n\n\nfig = go.Figure()\nfor gamma in [0, 0.5, 1, 2, 5, 10]:\n    fig.add_trace(go.Scatter(x=x, y=torch.log(ce(x)/fl(x, gamma)), mode='lines', name=f'gamma={gamma}'))\nfig.update_layout(title='log(Cross Entropy / Focal Loss)', xaxis_title='p', yaxis_title='log(Cross Entropy / Focal Loss)')\nfig.show()"
  },
  {
    "objectID": "blogs_madhu.html",
    "href": "blogs_madhu.html",
    "title": "Blogs",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 12, 2023\n\n\nReccomandation System\n\n\nMadhuri Awachar\n\n\n\n\nJun 5, 2023\n\n\nCUR DEcomposition\n\n\nMadhuri Awachar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html",
    "href": "blogs/blogsData/Reccomandation System.html",
    "title": "Reccomandation System",
    "section": "",
    "text": "Matching consumers with the most appropriate products is key to enhancing user satisfaction and loy- alty. Therefore, more retailers have become interested in recommender systems, which analyze patterns of user interest in products to provide personalized recommenda- tions that suit a user’s taste. Because good personalized recommendations can add another dimension to the user experience, e-commerce leaders like Amazon.com and Netflix have made recommender systems a salient part of their websites.\n\n\nModern consumers are inundated with choices. Electronic retailers and content providers offer a huge selection of prod- ucts, with unprecedented opportunities to meet a variety of special needs and tastes. Matching consumers with the most appropriate products is key to enhancing user satisfaction and loy- alty. Therefore, more retailers have become interested in recommender systems, which analyze patterns of user interest in products to provide personalized recommenda- tions that suit a user’s taste. Because good personalized recommendations can add another dimension to the user experience, e-commerce leaders like Amazon.com and Netflix have made recommender systems a salient part of their websites.\nCamera image:\n\n\n\n\nCollaborative filtering\nContent-based filtering\n\n\n\n\n1.An alternative to content filtering relies only on past user behavior—for example, previous transactions or product ratings— without requiring the creation of explicit profiles. This approach is known as collaborative filtering\n2.Collaborative filtering analyzes relationships between users and interdependencies among products to identify new user-item associations.\n3.more accurate than content-based techniques\n4.cold start problem, due to its inability to address the system’s new products and users. In this aspect, content filtering is superior."
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#why-super-resolution",
    "href": "blogs/blogsData/Reccomandation System.html#why-super-resolution",
    "title": "Reccomandation System",
    "section": "",
    "text": "High resolution image offers a high pixel density and thereby more details about the original scene. The need for high resolution is common in computer vision applications for better performance in pattern recognition and analysis of images. High resolution is of importance in medical imaging for diagnosis. Many applications require zooming of a specific area of interest in the image wherein high resolution becomes essential, e.g. surveillance, forensic and satellite imaging applications.\nHowever, high resolution images are not always available. This is since the setup for high resolution imaging proves expensive and also it may not always be feasible due to the inherent limitations of the sensor, optics manufacturing technology. These problems can be overcome through the use of image processing algorithms, which are relatively inexpensive, giving rise to concept of super-resolution. It provides an advantage as it may cost less and the existing low resolution imaging systems can still be utilized.\nCamera image:\n\n\n\nflower_blur_unblur.jpg\n\n\nSecurity camera image:\n\n\n\nnumberplate_blur_unblur.jpg\n\n\nGeological image:\n\n\n\ngeo_lr_hr.png"
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#how-we-create-our-dataset",
    "href": "blogs/blogsData/Reccomandation System.html#how-we-create-our-dataset",
    "title": "Reccomandation System",
    "section": "",
    "text": "We always do not have low and high resolution images of the same scene. So we create our own dataset. We take high resolution images (from net) and from that we create low resolution images by downscaling the high resolution images. We have used two methods for downscaling the images: 1. Bicubic Interpolation 2. Gaussian Blur\nWe won’t discussing in detail about these methods. We will be using the Gaussian Blur method for downscaling the images.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nimport cv2\n# Load MNIST dataset\n(x_train, _), (_, _) = keras.datasets.mnist.load_data()\n\n# Select a random image from the dataset\nimage = x_train[np.random.randint(0, x_train.shape[0])]\n\n# Normalize the pixel values\nimage = image.astype('float32') / 255.0\n\n# Blur the image using Gaussian filter\nblurred_image = cv2.GaussianBlur(image, (3, 3), 0)\n\n# Plot the original and blurred images\nplt.subplot(1, 2, 1)\nplt.imshow(image, cmap='gray')\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(blurred_image, cmap='gray')\nplt.title('Blurred Image')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nOne other quick way is to size down image then resize it to original size. This way we can get low resolution image."
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#but-why-cnn-why-not-mlp",
    "href": "blogs/blogsData/Reccomandation System.html#but-why-cnn-why-not-mlp",
    "title": "Reccomandation System",
    "section": "But why CNN? Why not MLP?",
    "text": "But why CNN? Why not MLP?\n\nMLPs (Multilayer Perceptron) use one perceptron for each input (e.g. pixel in an image) and the amount of weights rapidly becomes unmanageable for large images. It includes too many parameters because it is fully connected. Each node is connected to every other node in next and the previous layer, forming a very dense web — resulting in redundancy and inefficiency. As a result, difficulties arise whilst training and overfitting can occur which makes it lose the ability to generalize.\nAnother common problem is that MLPs react differently to an input (images) and its shifted version — they are not translation invariant.\nThe main problems is that spatial information is lost when the image is flattened(matrix to vector) into an MLP.\n\nBasic implementation of CNN and MLP on Mnist dataset for classification.(you can skip if you know basics about CNN and MLP)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Preprocess the data\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n\n# Reshape the data for MLP\nx_train_mlp = x_train.reshape((-1, 28*28))\nx_test_mlp = x_test.reshape((-1, 28*28))\n\n# Reshape the data for CNN\nx_train_cnn = x_train.reshape((-1, 28, 28, 1))\nx_test_cnn = x_test.reshape((-1, 28, 28, 1))\n\n\n\n\nMLP_fig_imageclassification.jpg\n\n\n\n# MLP model\nmlp_model = Sequential()\nmlp_model.add(Dense(256, activation='relu', input_shape=(28*28,)))\nmlp_model.add(Dense(128, activation='relu'))\nmlp_model.add(Dense(10, activation='softmax'))\nmlp_model.summary()\nmlp_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train MLP model\nmlp_model.fit(x_train_mlp, y_train, epochs=10, batch_size=32, validation_data=(x_test_mlp, y_test))\n\n# Evaluate MLP model\nmlp_loss, mlp_accuracy = mlp_model.evaluate(x_test_mlp, y_test)\nprint(\"MLP Accuracy:\", mlp_accuracy)\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_5 (Dense)             (None, 256)               200960    \n                                                                 \n dense_6 (Dense)             (None, 128)               32896     \n                                                                 \n dense_7 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 235146 (918.54 KB)\nTrainable params: 235146 (918.54 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/10\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.2051 - accuracy: 0.9395 - val_loss: 0.1189 - val_accuracy: 0.9636\nEpoch 2/10\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.0852 - accuracy: 0.9729 - val_loss: 0.0767 - val_accuracy: 0.9756\nEpoch 3/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0580 - accuracy: 0.9815 - val_loss: 0.0714 - val_accuracy: 0.9762\nEpoch 4/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0455 - accuracy: 0.9856 - val_loss: 0.0743 - val_accuracy: 0.9761\nEpoch 5/10\n1875/1875 [==============================] - 12s 7ms/step - loss: 0.0352 - accuracy: 0.9889 - val_loss: 0.0663 - val_accuracy: 0.9798\nEpoch 6/10\n1875/1875 [==============================] - 12s 7ms/step - loss: 0.0285 - accuracy: 0.9907 - val_loss: 0.0763 - val_accuracy: 0.9792\nEpoch 7/10\n1875/1875 [==============================] - 10s 6ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.0899 - val_accuracy: 0.9775\nEpoch 8/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 0.0878 - val_accuracy: 0.9792\nEpoch 9/10\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1028 - val_accuracy: 0.9746\nEpoch 10/10\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0165 - accuracy: 0.9941 - val_loss: 0.0898 - val_accuracy: 0.9791\n313/313 [==============================] - 1s 3ms/step - loss: 0.0898 - accuracy: 0.9791\nMLP Accuracy: 0.9790999889373779\n\n\n\n\n\nCNN_fig_imageclassification.jpg\n\n\n\n# CNN model\ncnn_model = Sequential()\ncnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\ncnn_model.add(MaxPooling2D((2, 2)))\ncnn_model.add(Flatten())\ncnn_model.add(Dense(128, activation='relu'))\ncnn_model.add(Dense(10, activation='softmax'))\ncnn_model.summary()\ncnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train CNN model\ncnn_model.fit(x_train_cnn, y_train, epochs=10, batch_size=32, validation_data=(x_test_cnn, y_test))\n\n# Evaluate CNN model\ncnn_loss, cnn_accuracy = cnn_model.evaluate(x_test_cnn, y_test)\nprint(\"CNN Accuracy:\", cnn_accuracy)\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_1 (Conv2D)           (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 13, 13, 32)        0         \n g2D)                                                            \n                                                                 \n flatten_1 (Flatten)         (None, 5408)              0         \n                                                                 \n dense_8 (Dense)             (None, 128)               692352    \n                                                                 \n dense_9 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 693962 (2.65 MB)\nTrainable params: 693962 (2.65 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/10\n1875/1875 [==============================] - 36s 19ms/step - loss: 0.1383 - accuracy: 0.9592 - val_loss: 0.0611 - val_accuracy: 0.9816\nEpoch 2/10\n1875/1875 [==============================] - 35s 19ms/step - loss: 0.0479 - accuracy: 0.9851 - val_loss: 0.0432 - val_accuracy: 0.9851\nEpoch 3/10\n1875/1875 [==============================] - 37s 20ms/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 0.0427 - val_accuracy: 0.9856\nEpoch 4/10\n1875/1875 [==============================] - 34s 18ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.0475 - val_accuracy: 0.9843\nEpoch 5/10\n1875/1875 [==============================] - 38s 20ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0455 - val_accuracy: 0.9862\nEpoch 6/10\n1875/1875 [==============================] - 40s 21ms/step - loss: 0.0103 - accuracy: 0.9965 - val_loss: 0.0448 - val_accuracy: 0.9864\nEpoch 7/10\n1875/1875 [==============================] - 42s 23ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0475 - val_accuracy: 0.9871\nEpoch 8/10\n1875/1875 [==============================] - 40s 21ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0526 - val_accuracy: 0.9857\nEpoch 9/10\n1875/1875 [==============================] - 41s 22ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 0.0576 - val_accuracy: 0.9856\nEpoch 10/10\n1875/1875 [==============================] - 43s 23ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0526 - val_accuracy: 0.9861\n313/313 [==============================] - 2s 6ms/step - loss: 0.0526 - accuracy: 0.9861\nCNN Accuracy: 0.9861000180244446\n\n\n\n# Select a random image from the test set\nindex = np.random.randint(0, x_test.shape[0])\n# Get the image and its label\nimage = x_test[index]\nlabel = y_test[index]\n# Reshape the image for MLP model prediction\nimage_mlp = image.reshape((1, 28*28))\n# Reshape the image for CNN model prediction\nimage_cnn = image.reshape((1, 28, 28, 1))\n# Predict using MLP model\nmlp_prediction = np.argmax(mlp_model.predict(image_mlp))\n# Predict using CNN model\ncnn_prediction = np.argmax(cnn_model.predict(image_cnn))\n# Plot the image and predictions\nplt.figure(figsize=(8, 4))\n\n# MLP plot\nplt.subplot(1, 2, 1)\nplt.imshow(image, cmap='gray')\nplt.title(f\"MLP Prediction: {mlp_prediction}\")\nplt.axis('off')\n\n# CNN plot\nplt.subplot(1, 2, 2)\nplt.imshow(image, cmap='gray')\nplt.title(f\"CNN Prediction: {cnn_prediction}\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n1/1 [==============================] - 0s 156ms/step\n1/1 [==============================] - 0s 82ms/step\n\n\n\n\n\nCNN does perform well on image classification task than MLP but above for toy dataset MNIST the difference is not that much. But for real world dataset the difference is huge.\nShow the difference between MLP and CNN for real world dataset.(open MLP vs CNN using transfer learning VGG16 on Snake vs Antelope dataset.ipynb)\nHere we can see that CNN is better than MLP for image classification. So we will be using CNN for our task."
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#unet-architecture-for-super-resolution",
    "href": "blogs/blogsData/Reccomandation System.html#unet-architecture-for-super-resolution",
    "title": "Reccomandation System",
    "section": "UNET architecture for super resolution:",
    "text": "UNET architecture for super resolution:\nHere we are just understand the high level view of unet architecture. We will be discussing in detail about unet in next blog. Unet has two parts: 1. Encoder 2. Decoder\nEncoder is same as CNN where we have convolutional layers and max-pooling layers. But in decoder we have upsampling layers instead of max-pooling layers. So the image is not shrinked and we get the high resolution image.\n\n\n\nu-net-architecture.png"
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#how-gans-works-analogy-counterfeiters-and-police",
    "href": "blogs/blogsData/Reccomandation System.html#how-gans-works-analogy-counterfeiters-and-police",
    "title": "Reccomandation System",
    "section": "How GANS works? Analogy: Counterfeiters and Police",
    "text": "How GANS works? Analogy: Counterfeiters and Police\n\nCounterfeiters: Generator\nPolice: Discriminator\n\nWe have an ambitious young criminal who wants to counterfeit money. He has a printing machine and he wants to print fake money. He has no idea how real money looks like. So he prints some money and goes to a shop to buy something. The shopkeeper is the discriminator. The shopkeeper knows how real money looks like. So he can easily identify the fake money. So the criminal goes back and prints some more money. This time the money looks more real. He goes to the shopkeeper again. The shopkeeper again identifies the fake money. This process continues until the criminal is able to print the exact replica of the real money. Now the shopkeeper is not able to identify the fake money. So the criminal is able to buy anything from the shopkeeper. The criminal has successfully fooled the shopkeeper. The criminal is the generator and the shopkeeper is the discriminator. This results in very realistic fake money. This is how GANS work.\nIn this sense both of them are getting better. The generator is getting better at generating fake money and the discriminator is getting better at identifying fake money. This is how GANS work. The generator generates fake images and the discriminator tries to identify the fake images. The generator tries to fool the discriminator and the discriminator tries to identify the fake images. This process continues until the discriminator is not able to identify the fake images. At this point the generator has successfully fooled the discriminator. The generator is now able to generate fake images which are indistinguishable from the real images.\nThis results in very realistic images. This is how GANS work.\n\nThe purpose of the generator Network is take random data initializations and decode it into synthetic sample\nThe purpose of the discriminator Network is to then take this input from our Generator and predict whether or not this sample came from the real dataset or not.\n\n\n\n\ngan_architecture.png"
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#training-gans",
    "href": "blogs/blogsData/Reccomandation System.html#training-gans",
    "title": "Reccomandation System",
    "section": "Training GANS",
    "text": "Training GANS\n\nTraining GANS is very difficult compared to Neural Networks we use gradient descent to change our weights and biases. But in GANS we have two networks generator and discriminator that works against eachother. So we have to train both of them simultaneously.\nWe are not seeking to minimize a loss function. We are seeking to find an equilibrium between the generator and discriminator.\nTraining stops when the discriminator is no longer able to distinguish between real and fake images.\n\n\nTraining process\n\nwe randomly generate a noisy vector\ninput this noisy vector into the generator to generate a fake image\nWe take some sample data from our real data and mix it with the fake data.\nWe train the discriminator to classifyf this mixed data as real or fake and update the weights of the discriminator.\nWe then train the generator. We make more random noisy vectors and create synthetic images. With the weights of the discriminator frozen, we use the feedbcak from the discriminator to update the weights of the generator.\n\nThis is how both Generator(to make better fake images) and Discriminator(to identify fake images) are getting better."
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#gans-for-super-resolution",
    "href": "blogs/blogsData/Reccomandation System.html#gans-for-super-resolution",
    "title": "Reccomandation System",
    "section": "GANS for Super Resolution",
    "text": "GANS for Super Resolution\nImporting necessary libraries\n\nfrom keras.datasets import mnist\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport glob\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, UpSampling2D\nfrom tensorflow.keras.layers import LeakyReLU, Dropout\nfrom tensorflow.keras.models import Sequential, Model, load_model\nimport time\n\n\n\n# a function to format display the losses\ndef hmsString(sec_elapsed):\n    h = int(sec_elapsed / (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) / 60)\n    s = sec_elapsed % 60\n    return \"{}:{:&gt;02}:{:&gt;05.2f}\".format(h, m, s)\n\n# downsample and introduce noise in the images\ndef downSampleAndNoisyfi(X):\n    shape = X[0].shape\n    X_down = []\n    for x_i in X:\n       x_c = cv2.resize(x_i, (shape[0]//4, shape[1]//4), interpolation = cv2.INTER_AREA)\n       x_c = np.clip(x_c+ np.random.normal(0, 5, x_c.shape) , 0, 255).astype('uint8')\n       X_down.append(x_c)\n    X_down = np.array(X_down, dtype = 'uint8')\n    return X_down\n\n\nCode for Generator Block\n\ndef Generator(input_shape):\n    X_input = Input(input_shape)\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X_input)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)  \n    X = Add()([X_shortcut, X])  \n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)  \n    X = Add()([X_shortcut, X])\n    X = Activation('relu')(X)\n    X = UpSampling2D(size=2)(X)\n    \n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X = Add()([X_shortcut, X])\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)   \n    X = Add()([X_shortcut, X])\n    X = Activation('relu')(X)\n    X = UpSampling2D(size=2)(X)\n    \n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(filters = 1, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    \n    generator_model = Model(inputs=X_input, outputs=X)\n    return generator_model\n\n\n\n\nCode for Discriminator Block\n\ndef Discriminator(input_shape):\n    X_input = Input(input_shape)\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X_input)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.8)(X)\n    X = Activation('relu')(X)\n    \n    discriminator_model = Model(inputs=X_input, outputs=X)\n    return discriminator_model\n\n\n\n\nTraing GANS\n\n# One step of the test step\n@tf.function\ndef train_step(X, Y, generator, discriminator, generator_optimizer, discriminator_optimizer):\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n    generated_images = generator(X, training=True)\n\n    real_output = discriminator(Y, training=True)\n    fake_output = discriminator(generated_images, training=False)\n\n    gen_loss = tf.keras.losses.MSE(Y, generated_images)\n    disc_loss = tf.keras.losses.MSE(real_output, fake_output)\n    \n\n    gradients_of_generator = gen_tape.gradient(\\\n        gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(\\\n        disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(\n        gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(\n        gradients_of_discriminator, \n        discriminator.trainable_variables))\n  return gen_loss,disc_loss\n\n# The main function to train the GAN\ndef train(X_train, Y_train, generator, discriminator, batch_size=100, epochs=50):\n    generator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    start = time.time()\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        gen_loss_list = []\n        disc_loss_list = []\n        \n        prev_i = 0\n        for i in range(X_train.shape[0]):\n            if((i+1)%batch_size == 0):\n                t = train_step(X_train[prev_i:i+1], Y_train[prev_i:i+1], generator, discriminator, generator_optimizer, discriminator_optimizer)\n                gen_loss_list.append(t[0])\n                disc_loss_list.append(t[1])\n                prev_i = i+1\n        g_loss = np.sum(np.array(gen_loss_list)) / np.sum(np.array(gen_loss_list).shape)\n        d_loss = np.sum(np.array(disc_loss_list)) / np.sum(np.array(disc_loss_list).shape)\n        \n        epoch_elapsed = time.time()-epoch_start\n        print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss}, {hmsString(epoch_elapsed)}')\n        \n    elapsed = time.time()-start\n    print (f'Training time: {hmsString(elapsed)}')\n    \n\n\n# loading the dataset(the original image are the HR 28*28 images)\n(Y_train, _), (Y_test, _) = mnist.load_data()\n# downsampling and introducing gaussian noise\n# this downsampled and noised dataset is out X or inputs\nX_train = downSampleAndNoisyfi(Y_train)\nX_test = downSampleAndNoisyfi(Y_test)\n\n# introduce a new dimension to the data (None, 28, 28, 1)\nX_test = X_test[..., np.newaxis]\nX_train = X_train[..., np.newaxis]\nY_train = Y_train[..., np.newaxis]\nY_test = Y_test[..., np.newaxis]\n\n# Creating a generator model\n# Showing the summary of generator \ngenerator = Generator((7,7,1))\ngenerator.summary()\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_1 (InputLayer)        [(None, 7, 7, 1)]            0         []                            \n                                                                                                  \n conv2d_2 (Conv2D)           (None, 7, 7, 32)             320       ['input_1[0][0]']             \n                                                                                                  \n batch_normalization (Batch  (None, 7, 7, 32)             128       ['conv2d_2[0][0]']            \n Normalization)                                                                                   \n                                                                                                  \n activation (Activation)     (None, 7, 7, 32)             0         ['batch_normalization[0][0]'] \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 7, 7, 32)             9248      ['activation[0][0]']          \n                                                                                                  \n batch_normalization_1 (Bat  (None, 7, 7, 32)             128       ['conv2d_3[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_1 (Activation)   (None, 7, 7, 32)             0         ['batch_normalization_1[0][0]'\n                                                                    ]                             \n                                                                                                  \n add (Add)                   (None, 7, 7, 32)             0         ['activation[0][0]',          \n                                                                     'activation_1[0][0]']        \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 7, 7, 32)             9248      ['add[0][0]']                 \n                                                                                                  \n batch_normalization_2 (Bat  (None, 7, 7, 32)             128       ['conv2d_4[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_2 (Activation)   (None, 7, 7, 32)             0         ['batch_normalization_2[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_1 (Add)                 (None, 7, 7, 32)             0         ['add[0][0]',                 \n                                                                     'activation_2[0][0]']        \n                                                                                                  \n activation_3 (Activation)   (None, 7, 7, 32)             0         ['add_1[0][0]']               \n                                                                                                  \n up_sampling2d (UpSampling2  (None, 14, 14, 32)           0         ['activation_3[0][0]']        \n D)                                                                                               \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 14, 14, 32)           9248      ['up_sampling2d[0][0]']       \n                                                                                                  \n batch_normalization_3 (Bat  (None, 14, 14, 32)           128       ['conv2d_5[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_4 (Activation)   (None, 14, 14, 32)           0         ['batch_normalization_3[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_6 (Conv2D)           (None, 14, 14, 32)           9248      ['activation_4[0][0]']        \n                                                                                                  \n batch_normalization_4 (Bat  (None, 14, 14, 32)           128       ['conv2d_6[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_5 (Activation)   (None, 14, 14, 32)           0         ['batch_normalization_4[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_2 (Add)                 (None, 14, 14, 32)           0         ['activation_4[0][0]',        \n                                                                     'activation_5[0][0]']        \n                                                                                                  \n conv2d_7 (Conv2D)           (None, 14, 14, 32)           9248      ['add_2[0][0]']               \n                                                                                                  \n batch_normalization_5 (Bat  (None, 14, 14, 32)           128       ['conv2d_7[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_6 (Activation)   (None, 14, 14, 32)           0         ['batch_normalization_5[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_3 (Add)                 (None, 14, 14, 32)           0         ['add_2[0][0]',               \n                                                                     'activation_6[0][0]']        \n                                                                                                  \n activation_7 (Activation)   (None, 14, 14, 32)           0         ['add_3[0][0]']               \n                                                                                                  \n up_sampling2d_1 (UpSamplin  (None, 28, 28, 32)           0         ['activation_7[0][0]']        \n g2D)                                                                                             \n                                                                                                  \n conv2d_8 (Conv2D)           (None, 28, 28, 32)           9248      ['up_sampling2d_1[0][0]']     \n                                                                                                  \n batch_normalization_6 (Bat  (None, 28, 28, 32)           128       ['conv2d_8[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_8 (Activation)   (None, 28, 28, 32)           0         ['batch_normalization_6[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_9 (Conv2D)           (None, 28, 28, 1)            289       ['activation_8[0][0]']        \n                                                                                                  \n batch_normalization_7 (Bat  (None, 28, 28, 1)            4         ['conv2d_9[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_9 (Activation)   (None, 28, 28, 1)            0         ['batch_normalization_7[0][0]'\n                                                                    ]                             \n                                                                                                  \n==================================================================================================\nTotal params: 56997 (222.64 KB)\nTrainable params: 56547 (220.89 KB)\nNon-trainable params: 450 (1.76 KB)\n__________________________________________________________________________________________________\n\n\n\n# Creating a discriminator model\n# Showing the summary of discriminator\ndiscriminator = Discriminator((28,28,1))\ndiscriminator.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d_10 (Conv2D)          (None, 28, 28, 32)        320       \n                                                                 \n activation_10 (Activation)  (None, 28, 28, 32)        0         \n                                                                 \n conv2d_11 (Conv2D)          (None, 28, 28, 64)        18496     \n                                                                 \n batch_normalization_8 (Bat  (None, 28, 28, 64)        256       \n chNormalization)                                                \n                                                                 \n activation_11 (Activation)  (None, 28, 28, 64)        0         \n                                                                 \n=================================================================\nTotal params: 19072 (74.50 KB)\nTrainable params: 18944 (74.00 KB)\nNon-trainable params: 128 (512.00 Byte)\n_________________________________________________________________\n\n\n\n# training with batch size of 100 and for 50 epochs\ntrain(X_train, Y_train, generator, discriminator, 100, 5) #50)\n\n# save the generator model for future use\ngenerator.save(\"mnist_generator_model\")\ngenerator.save(\"mnist_generator_model.h5\")\n\nHere we have run only for 5 epochs but you can run for more epochs to get better results.\n\n# testing the model\nY_pred = generator.predict(X_test)\n# showing the first 5 results\nfig,a =  plt.subplots(3,5)\nfig.subplots_adjust(hspace=0.5, wspace=0.1)\nfor i in range(5):\n    a[0][i].imshow(X_test[i])\n    a[0][i].axes.get_xaxis().set_visible(False)\n    a[0][i].axes.get_yaxis().set_visible(False)\n    a[0][i].title.set_text(\"LR: \"+str(i+1))\n    \n    a[1][i].imshow(Y_pred[i])\n    a[1][i].axes.get_xaxis().set_visible(False)\n    a[1][i].axes.get_yaxis().set_visible(False)\n    a[1][i].title.set_text(\"SR: \"+str(i+1)) \n    \n    a[2][i].imshow(Y_test[i])\n    a[2][i].axes.get_xaxis().set_visible(False)\n    a[2][i].axes.get_yaxis().set_visible(False)\n    a[2][i].title.set_text(\"HR: \"+str(i+1)) \n\n313/313 [==============================] - 9s 27ms/step\n\n\n\n\n\n\n# showing the first 5 random results\nimport random\nfigb,ab =  plt.subplots(3,5)\nfigb.subplots_adjust(hspace=0.5, wspace=0.1)\nfor i in range(5):\n    ii = random.randint(0, 10000) \n    \n    ab[0][i].imshow(X_test[ii])\n    ab[0][i].axes.get_xaxis().set_visible(False)\n    ab[0][i].axes.get_yaxis().set_visible(False)\n    ab[0][i].title.set_text(\"LR: \"+str(i+1))\n    \n    ab[1][i].imshow(Y_pred[ii])\n    ab[1][i].axes.get_xaxis().set_visible(False)\n    ab[1][i].axes.get_yaxis().set_visible(False)\n    ab[1][i].title.set_text(\"SR: \"+str(i+1)) \n    \n    ab[2][i].imshow(Y_test[ii])\n    ab[2][i].axes.get_xaxis().set_visible(False)\n    ab[2][i].axes.get_yaxis().set_visible(False)\n    ab[2][i].title.set_text(\"HR: \"+str(i+1)) \n\n\n\n\nWell GAN does perform good but it has some problems.\n\n\nProblem with GANS:\n\nAchieving equilibrium: between the generator and discriminator is very difficult.\nTime: Training gans is computationally expensive and necessitates tweaking of hyperparameters such as initializations, altering hidden layers, different activation, using Batch Normalization or Dropout, etc.\nBad Initializations: If the generator and discriminator are not initialized properly, then the training will fail.\nMode Collapse: happens when regardless of the nosie input fed into your generator, the generated output varies very little. It occurs when a small set of images look good to the descriminator and get scored better than other images. The GAN simple learns to reproduce those images over and over again. Analgous to overfittiing.\n\nOne quick solution to the problem of high training time is to use transfer learning using VGG16 or VGG19 in Generator and discriminator architecture. This will reduce the training time."
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html",
    "title": "Baysian Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#known-entities",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#known-entities",
    "title": "Baysian Linear Regression",
    "section": "Known entities",
    "text": "Known entities\n\nsigma = 1.0 # standard deviation of the noise\nm0 = 0.0 # mean of the prior\nS0 = 1.0 # covariance of the prior  \np = 6 # order of the polynomial \n\n\nN = 100 # number of data points\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, sigma) # training targets, size Nx1"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#posterior",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#posterior",
    "title": "Baysian Linear Regression",
    "section": "Posterior",
    "text": "Posterior\nCalculating\n\\[\\begin{align}\n&\\text{Parameter posterior: } p(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\mathcal N(\\boldsymbol \\theta \\,|\\, Mn,\\, Sn)\n\\end{align}\\]\n\ndef posterior(X, y, p, m0, S0, sigma):\n    \"\"\"Returns the posterior mean and covariance matrix of the weights given the training data.\"\"\"\n    poly_X = poly_features(X, p)\n\n    SN = scipy.linalg.inv(1.0 * np.eye(p+1) / S0  + 1.0/sigma**2 * poly_X.T @ poly_X)\n    mN = SN @ (m0 / S0 + (1.0/sigma**2) * poly_X.T @ y)    \n    \n    return mN, SN\n\n\nmN , SN = posterior(X, y, p ,m0, S0, sigma)\n\n\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\npoly_X_test = poly_features(Xtest, p)\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\nposterior_pred_mean = poly_X_test @ mN\n\nposterior_pred_uncertainty_para = poly_X_test @ SN @ poly_X_test.T\n\nposterior_pred_var = sigma**2 + posterior_pred_uncertainty_para\n\n\n# print(posterior_pred_mean.shape)\n# print(posterior_pred_var.shape)\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\n# plt.plot(Xtest, m_mle_test)\n# plt.plot(Xtest, m_map_test)\nposterior_pred_mean = posterior_pred_mean.flatten()\nvar_blr = np.diag(posterior_pred_uncertainty_para)\n\nconf_bound1 = np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound1, posterior_pred_mean - conf_bound1, alpha = 0.1, color=\"k\")\n\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound2, posterior_pred_mean - conf_bound2, alpha = 0.1, color=\"k\")\n\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound3, posterior_pred_mean - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.legend([\"Training data\",\"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#why-reccomadation-sys",
    "href": "blogs/blogsData/Reccomandation System.html#why-reccomadation-sys",
    "title": "Reccomandation System",
    "section": "",
    "text": "Modern consumers are inundated with choices. Electronic retailers and content providers offer a huge selection of prod- ucts, with unprecedented opportunities to meet a variety of special needs and tastes. Matching consumers with the most appropriate products is key to enhancing user satisfaction and loy- alty. Therefore, more retailers have become interested in recommender systems, which analyze patterns of user interest in products to provide personalized recommenda- tions that suit a user’s taste. Because good personalized recommendations can add another dimension to the user experience, e-commerce leaders like Amazon.com and Netflix have made recommender systems a salient part of their websites.\nCamera image:"
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#movie-reccomandation-example",
    "href": "blogs/blogsData/Reccomandation System.html#movie-reccomandation-example",
    "title": "Reccomandation System",
    "section": "Movie reccomandation example",
    "text": "Movie reccomandation example\nsystems are particularly useful for entertainment products such as movies, music, and TV shows. Many cus- tomers will view the same movie, and each customer is likely to view numerous different movies. Customers have proven willing to indicate their level of satisfaction with particular movies, so a huge volume of data is available about which movies appeal to which customers. Com- panies can analyze this data to recommend movies to particular customers."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html",
    "href": "blogs/blogsData/blr_blog.html",
    "title": "Baysian Linear Regression blog",
    "section": "",
    "text": "Welcome to my blog on Bayesian linear regression, where we explore the power of this technique. While traditional linear regression provides point estimates, Bayesian linear regression incorporates prior knowledge and quantifies uncertainty. By combining observed data with prior beliefs, we make more informed decisions. Throughout this blog, we’ll delve into key components like probalistic approch to linear regression, basics of types of uncertainity, prior distributions, likelihood functions, and posterior inference. Let’s embark on this enlightening journey together."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#maximum-likelihood",
    "href": "blogs/blogsData/blr_blog.html#maximum-likelihood",
    "title": "Baysian Linear Regression blog",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nWe will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta_{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta_{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X])\n\n\ndef max_lik_estimate(X, y):\n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\ntheta_ml = max_lik_estimate(X_aug,y)\nprint(theta_ml)\n\n[[2.116]\n [0.499]]\n\n\nNow we will make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), \\[\n\\ \\boldsymbol y_{\\text{pred}} = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta_{\\text{ML}}\n\\]\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\nml_prediction = Xtest_aug @ theta_ml\n\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\n\nText(0, 0.5, '$y$')\n\n\n\n\n\nThis gives fairly good results but what if the data is bit complex\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\nN = 10\nmu = 0\nsigma = 0.2**2\nseed(10)\nxn = np.random.uniform(-5, 5, N)\nepsilon = np.random.normal(mu, sigma, N)\nyn = -np.sin(xn/5) + np.cos(xn) + epsilon\ndataset = np.column_stack((xn, yn))\nxn = xn.reshape(-1,1)\nyn = yn.reshape(-1,1)\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\nLets first apply linear regressoin without non linear transformation\n\nN, D = xn.shape\nX_aug = np.hstack([np.ones((N,1)), xn]) # augmented training inputs of size N x (D+1)\n# theta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\n\ntheta_aug_ml = max_lik_estimate(X_aug, yn)\ntheta_aug_ml\n\narray([[-0.47109666],\n       [-0.1808517 ]])\n\n\n\nml_predictions = X_aug @ theta_aug_ml \n# X: K x D matrix of test inputs\n# theta: D x 1 vector of parameters\n# returns: prediction of f(X); K x 1 vector\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(xn, ml_predictions)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#polynomial-regression",
    "href": "blogs/blogsData/blr_blog.html#polynomial-regression",
    "title": "Baysian Linear Regression blog",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\nNonlinear Features\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nPolynomial Regression class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\ndef poly_features(X, p):\n    \"\"\"Returns a matrix with p columns containing the polynomial features of the input vector X.\"\"\"\n    X = X.flatten()\n    return np.array([1.0*X**i for i in range(p+1)]).T\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\np = 4\nPhi = poly_features(xn, p)\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\nX_test = np.linspace(-5,5, 100).reshape(-1,1)\nPhi_test =  poly_features(X_test, p)\ny_pred = Phi_test @ theta_ml\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(X_test, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\nNow lets try different polynomial fits.\n\n# Values of p to consider\np_values = [0, 1, 3, 4, 6, 9]\n\n# Create a 2x3 grid of subplots\nfig, axs = plt.subplots(2, 3, figsize=(12, 8))\n\nfor i, p in enumerate(p_values):\n\n    Phi = poly_features(xn, p)\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\n    Phi_test = poly_features(X_test, p)\n    y_pred = Phi_test @ theta_ml\n\n    ax = axs[i // 3, i % 3]  # Get the correct subplot\n    ax.plot(xn, yn, '+', markersize=10,label='Training data')\n    ax.plot(X_test, y_pred, label = 'MLE')\n    ax.set_xlabel(\"$x$\")\n    ax.set_ylabel(\"$y$\")\n    ax.set_ylim(-5, 5)\n    ax.set_xlim(-5, 5)\n    ax.set_title(f\"P = {p}\")\n    ax.legend()\n\n# Adjust the spacing between subplots\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\nYou can refer 9.1 and 9.2 section of Mathematics for Machine Learning to understand in depth about probalistic approch to linear regression."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayes-theroem",
    "href": "blogs/blogsData/blr_blog.html#bayes-theroem",
    "title": "Baysian Linear Regression blog",
    "section": "Bayes theroem",
    "text": "Bayes theroem\nThe basis of bayesian linear regression is bayes theroem. - Bayes’ theorem looks as follows: \\[\n\\begin{equation}\np(\\boldsymbol{\\theta} | \\mathbf{x}, y) = \\frac{p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\boldsymbol{x}, y)}\n\\end{equation}\n\\] - \\(p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})\\) is the likelihood. It describes the probability of the target values given the data and parameters. - \\(p(\\boldsymbol{\\theta})\\) is the prior. It describes our initial knowledge about which parameter values are likely and unlikely. - \\(p(\\boldsymbol{x}, y)\\) is the evidence. It describes the joint probability of the data and targets."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayesian-inference",
    "href": "blogs/blogsData/blr_blog.html#bayesian-inference",
    "title": "Baysian Linear Regression blog",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nIn general, Bayesian inference works as follows: 1. We start with some prior belief about a hypothesis \\(p(h)\\) 2. We observe some data, representating new evidence \\(e\\) 3. We use Bayes’ theorem to update our belief given the new evidence: \\(p(h|e) = \\frac{p(e |h)p(h)}{p(e)}\\)\nHave a look at Wiki"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayeisan-approch",
    "href": "blogs/blogsData/blr_blog.html#bayeisan-approch",
    "title": "Baysian Linear Regression blog",
    "section": "Bayeisan approch",
    "text": "Bayeisan approch\nUnlike linear regression where we computed point estimates of our parameters using maximum likelihood approach and make predictions, here in Bayesian linear regression we estimate\nFollowing are the steps: 1. We assume a that we know standard deviation of the noise, mean and covariance of the prior. 2. We than calculate parameter posteriori 3. Based on that we make posteriori predictions on unseen data ie. test data.\nNow lets see along with code\nHere we have same assumptions that we took in linear regression \\[\ny = \\boldsymbol x^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n\\] Where epsilon is the noise from normal distribution with variance \\(\\sigma^2\\). Training inputs in \\(\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}\\) and corresponding training targets \\(\\mathcal Y = \\{y_1, \\ldots, y_N\\}\\), respectively.\nFunction\n\ndef g(x, mu, sigma):   \n    epsilon = np.random.normal(mu, sigma, size=(x.shape))\n    return np.cos(x) + epsilon\n    \n\nWe apply non linear feature transformation on feature matrix with polynomial of degree \\(K\\) \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nSample to see nonlinear transformation\n\nX = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\n\n\npoly_features(X, 3) # defined in linear regression section\n\narray([[  1.,  -3.,   9., -27.],\n       [  1.,  -1.,   1.,  -1.],\n       [  1.,   0.,   0.,   0.],\n       [  1.,   1.,   1.,   1.],\n       [  1.,   3.,   9.,  27.]])"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#known-entities",
    "href": "blogs/blogsData/blr_blog.html#known-entities",
    "title": "Baysian Linear Regression blog",
    "section": "Known entities",
    "text": "Known entities\n\nsigma = 1.0 # standard deviation of the noise\nm0 = 0.0 # mean of the prior\nS0 = 1.0 # covariance of the prior  \np = 6 # order of the polynomial \n\n\\[\n\\boxed{\\begin{array}{l}\n\\ \\ \\ \\ \\ \\ \\ \\ \\ m_{0} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ S_{0}\\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\searrow \\ \\ \\ \\swarrow \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\theta \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\downarrow \\ \\ \\ \\swarrow \\\\\n\\ \\ \\ \\ x_{n} \\ \\ \\rightarrow \\ \\ y_{n} \\ \\ \\\\\n\\ \\ \\ \\ n\\ =\\ 1,......,N\\ \\\\\n\\end{array}}\n\\] \\[ Graphical \\ model \\ for \\ Bayeisan \\ linear \\ regression \\]\n\nN = 100 # number of data points\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, m0, sigma) # training targets, size Nx1"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#posterior",
    "href": "blogs/blogsData/blr_blog.html#posterior",
    "title": "Baysian Linear Regression blog",
    "section": "Posterior",
    "text": "Posterior\n\nParameter posteriori in closed form\nCalculating Parameter posterior: \\[\n\\begin{aligned}\np(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y}) &=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\\\\n\\boldsymbol{S}_{N} &=\\left(\\boldsymbol{S}_{0}^{-1}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{\\Phi}\\right)^{-1} \\\\\n\\boldsymbol{m}_{N} &=\\boldsymbol{S}_{N}\\left(\\boldsymbol{S}_{0}^{-1} \\boldsymbol{m}_{0}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{y}\\right)\n\\end{aligned}\n\\]\n\ndef posterior(X, y, p, m0, S0, sigma):\n    \"\"\"Returns the posterior mean and covariance matrix of the weights given the training data.\"\"\"\n    poly_X = poly_features(X, p)\n\n    SN = scipy.linalg.inv(1.0 * np.eye(p+1) / S0  + 1.0/sigma**2 * poly_X.T @ poly_X)\n    mN = SN @ (m0 / S0 + (1.0/sigma**2) * poly_X.T @ y)    \n    \n    return mN, SN\n\n\nmN , SN = posterior(X, y, p ,m0, S0, sigma)\n\n\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\npoly_X_test = poly_features(Xtest, p)\n\n\n\nPosterior Predictive distribution\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\n\\begin{align}\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}}) \\, |\\ X, Y, \\boldsymbol X_{\\text{test}}) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol M_{\\text{n}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol S_{\\text{N}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top + \\sigma ^ 2)\n\\end{align} \\] We already computed all quantities. Write some code that implements all three predictors.\n\nposterior_pred_mean = poly_X_test @ mN\n\nposterior_pred_uncertainty_para = poly_X_test @ SN @ poly_X_test.T\n\nposterior_pred_var = sigma**2 + posterior_pred_uncertainty_para\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\n# plt.plot(Xtest, m_mle_test)\n# plt.plot(Xtest, m_map_test)\nposterior_pred_mean = posterior_pred_mean.flatten()\nvar_blr = np.diag(posterior_pred_uncertainty_para)\n\n# conf_bound1 = np.sqrt(var_blr).flatten()\n# plt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound1, posterior_pred_mean - conf_bound1, alpha = 0.1, color=\"k\")\n\n# 95 % parameter uncertainity\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound2, posterior_pred_mean - conf_bound2, alpha = 0.1, color=\"r\")\n\n# 95 % total uncertainity ie. \nconf_bound3 = 2.0*np.sqrt(var_blr + sigma**2).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound3, posterior_pred_mean - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.legend([\"Training data\", '95% para uncertainity', '95% total uncertainity'])\nplt.xlabel('$x$');\nplt.ylabel('$y$');\n\n\n\n\nYou can refer 9.3 section of Mathematics for Machine Learning to understand in depth about bayesian linear regression.\n\n\nVisulizing the parameter Posterior\nIn this section we will visualize the posterior and will see how it changes as it sees more data.\n\ndef f(x, a): return a[0] + a[1] * x\n\n\ndef plot_prior(m, S, liminf=-1, limsup=1, step=0.05, ax=plt, **kwargs):\n    grid = np.mgrid[liminf:limsup + step:step, liminf:limsup + step:step]\n    nx = grid.shape[-1]\n    z = multivariate_normal.pdf(grid.T.reshape(-1, 2), mean=m.ravel(), cov=S).reshape(nx, nx).T\n    \n    return ax.contourf(*grid, z, **kwargs)\n\ndef plot_sample_w(mean, cov, size=10, ax=plt):\n    w = np.random.multivariate_normal(mean=mean.ravel(), cov=cov, size=size)\n    x = np.linspace(-1, 1)\n    for wi in w:\n        ax.plot(x, f(x, wi), c=\"tab:blue\", alpha=0.4)\n\ndef plot_likelihood_obs(X, T, ix, ax=plt):\n    \"\"\"\n    Plot the likelihood function of a single observation\n    \"\"\"\n    W = np.mgrid[-1:1:0.1, -1:1:0.1]\n    x, t = sample_vals(X, T, ix) # ith row\n    mean = W.T.reshape(-1, 2) @ x.T\n\n    likelihood = norm.pdf(t, loc=mean, scale= np.sqrt(sigma **2)).reshape(20, 20).T\n    ax.contourf(*W, likelihood)\n    ax.scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n\ndef sample_vals(X, T, ix):\n    \"\"\"\n    \n    Returns\n    -------\n    Phi: The linear model transormation\n    t: the target datapoint\n    return ith data\n    \"\"\"\n    x_in = X[ix]\n    Phi = np.c_[np.ones_like(x_in), x_in]\n    t = T[[ix]]\n    return Phi, t\n\ndef posterior_w(phi, t, S0, m0):\n    \"\"\"\n    Compute the posterior distribution of \n    a Gaussian with known precision and conjugate\n    prior a gaussian\n    \n    Parameters\n    ----------\n    phi: np.array(N, M)\n    t: np.array(N, 1)\n    S0: np.array(M, M)\n        The prior covariance matrix\n    m0: np.array(M, 1)\n        The prior mean vector\n    \"\"\"\n    SN = inv(inv(S0) + ((1 / sigma) ** 2) * phi.T @ phi)\n    mN = SN @ (inv(S0) @ m0 + ((1 / sigma) ** 2) * phi.T @ t)\n    return SN, mN\n\n\nseed(314)\na = np.array([-0.3, 0.5]) # true parameter values\nN = 30\nsigma = 0.2\nX = uniform(-1, 1, (N, 1))\nT = f(X, a) + randn(N, 1) * sigma\n\n\n# beta = (1 / sigma) ** 2 # precision\nalpha = 2.0\n\n\nSN = np.eye(2) / alpha\nmN = np.zeros((2, 1))\nseed(1643)\n\n\nnobs = [1, 5, 15, 30]\nix_fig = 1\nfig, ax = plt.subplots(len(nobs) + 1, 3, figsize=(10, 12))\nplot_prior(mN, SN, ax=ax[0,1])\nax[0, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\nax[0, 0].axis(\"off\")\nplot_sample_w(mN, SN, ax=ax[0, 2])\nfor i in range(0, N):\n    Phi, t = sample_vals(X, T, i)\n    SN, mN = posterior_w(Phi, t, SN, mN)\n    if i+1 in nobs:\n        plot_likelihood_obs(X, T, i, ax=ax[ix_fig, 0])\n        plot_prior(mN, SN, ax=ax[ix_fig, 1])\n        ax[ix_fig, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n        ax[ix_fig, 2].scatter(X[:i + 1], T[:i + 1], c=\"crimson\")\n        ax[ix_fig, 2].set_xlim(-1, 1)\n        ax[ix_fig, 2].set_ylim(-1, 1)\n        for l in range(2):\n            ax[ix_fig, l].set_xlabel(\"$$\\theta_0$$\")\n            ax[ix_fig, l].set_ylabel(\"$$\\theta_1$$\")\n        plot_sample_w(mN, SN, ax=ax[ix_fig, 2])\n        ix_fig += 1\n\ntitles = [\"likelihood\", \"prior/posterior\", \"data space\"]\nfor axi, title in zip(ax[0], titles):\n    axi.set_title(title, size=15)\nplt.tight_layout()\n\n\n\n\nWe can see above as the model see more data, the posterior converges close the the true values at end. Refer to Bishop - Pattern Recognition and Machine Learning fig 3.7 to understand above fig in detail."
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#recommender-system-strategies",
    "href": "blogs/blogsData/Reccomandation System.html#recommender-system-strategies",
    "title": "Reccomandation System",
    "section": "",
    "text": "Collaborative filtering\nContent-based filtering"
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#collaborative-filtering",
    "href": "blogs/blogsData/Reccomandation System.html#collaborative-filtering",
    "title": "Reccomandation System",
    "section": "",
    "text": "1.An alternative to content filtering relies only on past user behavior—for example, previous transactions or product ratings— without requiring the creation of explicit profiles. This approach is known as collaborative filtering\n2.Collaborative filtering analyzes relationships between users and interdependencies among products to identify new user-item associations.\n3.more accurate than content-based techniques\n4.cold start problem, due to its inability to address the system’s new products and users. In this aspect, content filtering is superior."
  },
  {
    "objectID": "blogs/blogsData/Reccomandation System.html#matrix-factorization-methods-for-reccomandation-system",
    "href": "blogs/blogsData/Reccomandation System.html#matrix-factorization-methods-for-reccomandation-system",
    "title": "Reccomandation System",
    "section": "Matrix Factorization Methods for reccomandation system",
    "text": "Matrix Factorization Methods for reccomandation system\nOne other quick way is to size down image then resize it to original size. This way we can get low resolution image."
  },
  {
    "objectID": "blogs/blogsData/CUR_Decomposition.html",
    "href": "blogs/blogsData/CUR_Decomposition.html",
    "title": "CUR DEcomposition",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport math\nimport pickle\n\nFunction\n\nDATASET\n\ndef get_user_movie_rating_matrix(file_path):\n    \"\"\"\n    Loads data from the specified file and constructs the user_movie_rating_matrix\n\n    Args:\n        file_path (str): Path to the data file.\n\n    Returns:\n        user_movie_rating_matrix (np.ndarray): User-movie rating matrix.\n    \"\"\"\n    df = pd.read_csv(file_path, sep=\"\\t\", names=['user_id', 'movie_id', 'rating', 'timestamp'])\n    num_users = df['user_id'].max()\n    num_movies = df['movie_id'].max()\n\n    user_movie_rating_matrix = np.zeros((num_users, num_movies))\n\n    for i in range(len(df)):\n        user_id = int(df['user_id'][i]) - 1\n        movie_id = int(df['movie_id'][i]) - 1\n        rating = float(df['rating'][i])\n\n        user_movie_rating_matrix[user_id][movie_id] = rating\n\n    return user_movie_rating_matrix\n\n\nfile_path = '/Users/madhuriawachar/Downloads/madhuriawachar1.github.io/ml-100k/u.data'  # Specify the correct file path to your data file\nuser_movie_matrix = get_user_movie_rating_matrix(file_path)\n\nprint('user_movie_matrix shape:', user_movie_matrix.shape)\nprint('user_movie_matrix:', user_movie_matrix)\n\nuser_movie_matrix shape: (943, 1682)\nuser_movie_matrix: [[5. 3. 4. ... 0. 0. 0.]\n [4. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [5. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 5. 0. ... 0. 0. 0.]]\n\n\n\n\nCUR Decomposition\n\nclass CUR():\n    \"\"\"\n        Predicts the ratings of first quater of user movie matrix using CUR\n    \"\"\"\n\n    def __init__(self, rating_matrix,epochs, num_factors=50, learning_rate=0.01, regularization=0.1):\n        self.rating_matrix = rating_matrix\n        self.num_users, self.num_movies = rating_matrix.shape\n        self.num_factors = num_factors\n        self.learning_rate = learning_rate\n        self.regularization = regularization\n        self.epochs = epochs\n\n        # Initialize matrices C, U, and R with random values\n        self.C = np.random.rand(self.num_users, num_factors)\n        self.U = np.random.rand(num_factors, self.num_movies)\n        self.R = np.random.rand(num_factors, self.num_movies)\n\n        self.generated_rating_matrix = self.cur()\n\n    def svd(self, matrix, k):\n        \"\"\"\n        Performs the SVD decomposition on the input matrix\n\n        Args:\n            matrix (np.ndarray) : The user rating matrix\n            k (int) : the reduced dimensionality after decomposition\n\n        Returns:\n            The three SVD matrices U,Sigma and V_T\n\n        \"\"\"\n        m = matrix.shape[0]\n        n = matrix.shape[1]\n\n        if (k &gt; m) or (k &gt; n):\n            print(\"error: k greater than matrix dimensions.\\n\")\n            return\n\n        matrix_t = matrix.T\n\n        A = np.dot(matrix, matrix_t)  # calculate matrix multiplied by its transpose\n        values1, v1 = np.linalg.eigh(A)  # get eigenvalues and eigenvectors\n        v1_t = v1.T\n        # discarding negative eigenvalues and corresponding eigenvectors (they are anyway tending to zero)\n        v1_t[values1 &lt; 0] = 0\n        v1 = v1_t.T\n        values1[values1 &lt; 0] = 0\n        # values1 = np.absolute(values1)\n\n        values1 = np.sqrt(values1)  # finding singular values.\n        # sort eigenvalues and eigenvectors in decreasing order\n        idx = np.argsort(values1)\n        idx = idx[::-1]\n        values1 = values1[idx]\n        v1 = v1[:, idx]\n\n        U = v1\n\n        A = np.dot(matrix_t, matrix)  # calculate matrix transpose multiplied by matrix.\n        values2, v2 = np.linalg.eigh(A)  # get eigenvalues and eigenvectors\n        # values2 = np.absolute(values2)\n        # discarding negative eigenvalues and corresponding eigenvectors(they are anyway tending to zero)\n        v2_t = v2.T\n        v2_t[values2 &lt; 0] = 0\n        v2 = v2_t.T\n        values2[values2 &lt; 0] = 0\n\n        values2 = np.sqrt(values2)  # finding singular values.\n        # sort eigenvalues and eigenvectors in decreasing order.\n        idx = np.argsort(values2)\n        idx = idx[::-1]\n        values2 = values2[idx]\n        v2 = v2[:, idx]\n\n        V = v2\n        V_t = V.T  # taking V transpose.\n\n        sigma = np.zeros((m, n))\n\n        if m &gt; n:  # setting the dimensions of sigma matrix.\n\n            sigma[:n, :] = np.diag(values2)\n\n        elif n &gt; m:\n            sigma[:, :m] = np.diag(values1)\n\n        else:\n            sigma[:, :] = np.diag(values1)\n\n        if m &gt; k:  # slicing the matrices according to the k value.\n            U = U[:, :k]\n            sigma = sigma[:k, :]\n\n        if n &gt; k:\n            V_t = V_t[:k, :]\n            sigma = sigma[:, :k]\n\n        check = np.dot(matrix, V_t.T)\n        # case = np.divide(check, values2[:k])\n\n        s1 = np.sign(check)\n        s2 = np.sign(U)\n        c = s1 == s2\n        # choosing the correct signs of eigenvectors in the U matrix.\n        for i in range(U.shape[1]):\n            if c[0, i] is False:\n                U[:, i] = U[:, i] * -1\n\n        return U, sigma, V_t\n    \n\n    def cur(self):\n        sample_size = 100\n\n        # Sampling columns - C\n        matrix_sum = (self.rating_matrix**2).sum()\n        col_prob = (self.rating_matrix**2).sum(axis=0)\n        col_prob /= matrix_sum\n\n        col_indices = np.random.choice(np.arange(0,self.num_movies), size=sample_size, replace=True, p=col_prob)\n        self.C = self.rating_matrix.copy()[:,col_indices]\n        self.C = np.divide(self.C,(sample_size*col_prob[col_indices])**0.5)\n\n        # Sampling rows - R\n        row_prob = (self.rating_matrix**2).sum(axis=1)\n        row_prob /= matrix_sum\n\n        row_indices = np.random.choice(np.arange(0,self.num_users), size=sample_size, replace=True, p=row_prob)\n        self.R = self.rating_matrix.copy()[row_indices, :]\n        self.R = np.divide(self.R, np.array([(sample_size*row_prob[row_indices])**0.5]).transpose())\n\n        # Finding U\n\n        # W - intersection of sampled C and R\n        W = self.rating_matrix.copy()[:, col_indices]\n        W = W[row_indices, :]\n\n        X, Z, YT = self.svd(W,50)\n\n        for i in range(min(Z.shape[0],Z.shape[1])):\n            if (Z[i][i] != 0):\n                Z[i][i] = 1/Z[i][i]\n\n        Y = YT.transpose()\n        XT = X.transpose()\n\n        self.U = Y.dot(Z.dot(XT))\n        \n       \n        # CUR = C * U * R\n        '''print('shapes')\n        print(C.shape)\n        print(U.shape)  \n        print(R.shape)'''\n        reconstructedMatrix = np.dot(self.C,self.U)\n        reconstructedMatrix = np.dot(reconstructedMatrix,self.R)\n        #print(reconstructedMatrix)\n        CUR_matrix=reconstructedMatrix\n        #CUR_matrix = C.dot(U.dot(R))\n       \n        return CUR_matrix\n\n    def predict_rating(self, x, i):\n        return max(0, min(5, abs(self.generated_rating_matrix.item((x,i)))))\n\n\ndef rmse_spearmans_rank_correlation(recommender):\n    \"\"\"\n        Root mean square error and Spearman's rank correlation\n        Lower the RMSE and rank correlation close to 1, better the algorithm\n    \"\"\"\n    diff = 0.0\n    num_pred = 0\n    rows = recommender.num_users // 4\n    cols = recommender.num_movies // 4\n    for i in range(rows):\n        for j in range(cols):    \n            if recommender.rating_matrix[i][j] != 0:\n                \n                    \n                diff += ((recommender.predict_rating(i, j) -\n                          recommender.rating_matrix[i][j])**2)\n                num_pred += 1\n\n                # print(recommender.predict_rating(i, j))\n                # print(recommender.rating_matrix[i][j])\n\n    rmse = math.sqrt(diff/num_pred)\n    rankcor = 1-((6*diff)/(num_pred*((num_pred**2)-1)))\n\n    print(type(recommender).__name__, 'RMSE', rmse)\n\n\nrecommender = CUR(user_movie_matrix, 100)\n \nrmse_spearmans_rank_correlation(recommender)\n\nCUR RMSE 1.9234296170240126"
  },
  {
    "objectID": "blogs/blogsData/CUR_Decomposition.html#known-entities",
    "href": "blogs/blogsData/CUR_Decomposition.html#known-entities",
    "title": "Baysian Linear Regression",
    "section": "Known entities",
    "text": "Known entities\n\nsigma = 1.0 # standard deviation of the noise\nm0 = 0.0 # mean of the prior\nS0 = 1.0 # covariance of the prior  \np = 6 # order of the polynomial \n\n\nN = 100 # number of data points\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, sigma) # training targets, size Nx1"
  },
  {
    "objectID": "blogs/blogsData/CUR_Decomposition.html#posterior",
    "href": "blogs/blogsData/CUR_Decomposition.html#posterior",
    "title": "Baysian Linear Regression",
    "section": "Posterior",
    "text": "Posterior\nCalculating\n\\[\\begin{align}\n&\\text{Parameter posterior: } p(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\mathcal N(\\boldsymbol \\theta \\,|\\, Mn,\\, Sn)\n\\end{align}\\]\n\ndef posterior(X, y, p, m0, S0, sigma):\n    \"\"\"Returns the posterior mean and covariance matrix of the weights given the training data.\"\"\"\n    poly_X = poly_features(X, p)\n\n    SN = scipy.linalg.inv(1.0 * np.eye(p+1) / S0  + 1.0/sigma**2 * poly_X.T @ poly_X)\n    mN = SN @ (m0 / S0 + (1.0/sigma**2) * poly_X.T @ y)    \n    \n    return mN, SN\n\n\nmN , SN = posterior(X, y, p ,m0, S0, sigma)\n\n\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\npoly_X_test = poly_features(Xtest, p)\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\nposterior_pred_mean = poly_X_test @ mN\n\nposterior_pred_uncertainty_para = poly_X_test @ SN @ poly_X_test.T\n\nposterior_pred_var = sigma**2 + posterior_pred_uncertainty_para\n\n\n# print(posterior_pred_mean.shape)\n# print(posterior_pred_var.shape)\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\n# plt.plot(Xtest, m_mle_test)\n# plt.plot(Xtest, m_map_test)\nposterior_pred_mean = posterior_pred_mean.flatten()\nvar_blr = np.diag(posterior_pred_uncertainty_para)\n\nconf_bound1 = np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound1, posterior_pred_mean - conf_bound1, alpha = 0.1, color=\"k\")\n\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound2, posterior_pred_mean - conf_bound2, alpha = 0.1, color=\"k\")\n\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound3, posterior_pred_mean - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.legend([\"Training data\",\"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  }
]